{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Class 17: Probabilities & Classification </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import brier_score_loss, roc_curve, auc, confusion_matrix, roc_auc_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(y_true, y_pred):\n",
    "\n",
    "    # Regression metrics\n",
    "    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n",
    "    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n",
    "    mse=metrics.mean_squared_error(y_true, y_pred) \n",
    "    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n",
    "    r2=metrics.r2_score(y_true, y_pred)\n",
    "\n",
    "    print('explained_variance: ', round(explained_variance,4))    \n",
    "    print('r2: ', round(r2,4))\n",
    "    print('MAE: ', round(mean_absolute_error,4))\n",
    "    print('MSE: ', round(mse,4))\n",
    "    print('RMSE: ', round(np.sqrt(mse),4))\n",
    "    \n",
    "def create_coef_matrix(X, model):\n",
    "    coef_matrix = pd.concat(\n",
    "        [pd.DataFrame(X.columns),pd.DataFrame(model.coef_.flatten())], axis = 1\n",
    "    )\n",
    "    coef_matrix.columns = ['variable', 'coefficient']\n",
    "    coef_matrix.iloc[-1] = ['Intercept', model.intercept_.flatten()[0]]\n",
    "    return coef_matrix\n",
    "\n",
    "def cv_summary(lambdas, C_values, model):\n",
    "    d = {'lambdas': lambdas, \n",
    "         'C_values': C_values, \n",
    "         'mean_cv_score': model.scores_[1].mean(axis = 0)}\n",
    "    return(pd.DataFrame(data=d))\n",
    "\n",
    "\"\"\"def create_roc_plot(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    all_coords = pd.DataFrame({\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    \n",
    "    plot = ggplot(all_coords, aes(x = 'fpr', y = 'tpr')) \\\n",
    "        + geom_line(color=color[0], size = 0.7) \\\n",
    "        + geom_area(position = 'identity', fill = 'mediumaquamarine', alpha = 0.3) \\\n",
    "        + xlab(\"False Positive Rate (1-Specifity)\") \\\n",
    "        + ylab(\"True Positive Rate (Sensitivity)\") \\\n",
    "        + geom_abline(intercept = 0, slope = 1,  linetype = \"dotted\", color = \"black\") \\\n",
    "        + scale_y_continuous(limits = (0, 1), breaks = seq(0, 1, .1), expand = (0, 0.01)) \\\n",
    "        + scale_x_continuous(limits = (0, 1), breaks = seq(0, 1, .1), expand = (0.01, 0)) \\\n",
    "        + theme_bw()\n",
    "    return(plot)\n",
    "\"\"\"\n",
    "\n",
    "def create_roc_plot(y_true, y_pred):\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Plot ROC curve line\n",
    "    ax.plot(fpr, tpr, color='k', linewidth=0.7)\n",
    "    \n",
    "    # Fill area under curve\n",
    "    ax.fill_between(fpr, tpr, alpha=0.3, color='white')\n",
    "    \n",
    "    # Add diagonal dotted line\n",
    "    ax.plot([0, 1], [0, 1], linestyle=':', color='black')\n",
    "    \n",
    "    # Set axis labels\n",
    "    ax.set_xlabel('False Positive Rate (1-Specificity)')\n",
    "    ax.set_ylabel('True Positive Rate (Sensitivity)')\n",
    "    \n",
    "    # Set axis limits and ticks\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    \n",
    "    # Style similar to theme_bw()\n",
    "    ax.grid(True, linestyle='-', alpha=0.2)\n",
    "    ax.set_facecolor('white')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def sigmoid_array(x):\n",
    "    return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def generate_fold_prediction(model, X, fold, param_index):\n",
    "    fold_coef = model.coefs_paths_[1][fold,param_index,:]\n",
    "    return(sigmoid_array(np.dot(X, np.transpose(fold_coef)[:-1]) +  np.transpose(fold_coef)[-1]))\n",
    "\n",
    "\"\"\"def create_loss_plot(all_coords, optimal_threshold, curr_exp_loss):\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['loss'] = (all_coords_copy.false_pos*FP + all_coords_copy.false_neg*FN)/all_coords_copy.n\n",
    "    \n",
    "    t = optimal_threshold\n",
    "    l = curr_exp_loss\n",
    "    \n",
    "    plot = ggplot(all_coords_copy, aes(x = 'thresholds', y = 'loss')) + \\\n",
    "        geom_line(color=color[0], size=0.7) + \\\n",
    "        scale_x_continuous(breaks = seq(0, 1.1, by = 0.1)) + \\\n",
    "        coord_cartesian(xlim=(0,1))+ \\\n",
    "        geom_vline(xintercept = t , color = color[0] ) + \\\n",
    "        annotate(geom = \"text\", x = t - 0.01, y= max(all_coords_copy.loss) - 0.4,\n",
    "                 label=\"best threshold: \" + str(round(t,2)),\n",
    "                 colour=color[1], angle=90, size = 7) +\\\n",
    "        annotate(geom = \"text\", x = t + 0.06, y= l,\\\n",
    "                 label= str(round(l, 2)), size = 7) +\\\n",
    "        theme_bw()\n",
    "    return(plot)\"\"\"\n",
    "\n",
    "def create_loss_plot(all_coords, optimal_threshold, curr_exp_loss):\n",
    "    # Create copy and calculate loss\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['loss'] = (all_coords_copy.false_pos*FP + all_coords_copy.false_neg*FN)/all_coords_copy.n\n",
    "    \n",
    "    t = optimal_threshold\n",
    "    l = curr_exp_loss\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    # Plot loss line\n",
    "    ax.plot(all_coords_copy['thresholds'], all_coords_copy['loss'], \n",
    "            color= 'k', linewidth=0.7)\n",
    "\n",
    "    # Add vertical line at optimal threshold\n",
    "    ax.axvline(x=t, color = 'k')\n",
    "\n",
    "    # Add annotations\n",
    "    ax.text(t - 0.04, max(all_coords_copy.loss) - 0.5,\n",
    "            f\"best threshold: {t:.2f}\", \n",
    "            color = 'k', \n",
    "            rotation=90, \n",
    "            fontsize = 9)\n",
    "    \n",
    "    ax.text(t + 0.06, l,\n",
    "            f\"{l:.2f}\",\n",
    "            fontsize = 9)\n",
    "\n",
    "    # Set x-axis ticks and limits\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "    # Style similar to theme_bw()\n",
    "    ax.grid(True, linestyle='-', alpha=0.2)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_xlabel('threshold')\n",
    "    ax.set_ylabel('loss')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"def create_roc_plot_with_optimal(all_coords, optimal_threshold):\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['sp'] = all_coords_copy.true_neg/all_coords_copy.neg\n",
    "    all_coords_copy['se'] = all_coords_copy.true_pos/all_coords_copy.pos\n",
    "    \n",
    "    best_coords = all_coords_copy[all_coords_copy.thresholds == optimal_threshold]\n",
    "    sp = best_coords.sp.values[0]\n",
    "    se = best_coords.se.values[0]\n",
    "\n",
    "    plot = ggplot(all_coords_copy, aes(x = 'sp', y = 'se')) +\\\n",
    "        geom_line(color=color[0], size=0.7) +\\\n",
    "        scale_y_continuous(breaks = seq(0, 1.1, by = 0.1)) +\\\n",
    "        scale_x_reverse(breaks = seq(0, 1.1, by = 0.1)) +\\\n",
    "        geom_point(data = pd.DataFrame({'sp': [sp], 'se': [se]})) +\\\n",
    "        annotate(geom = \"text\", x = sp, y = se + 0.03,\n",
    "                 label = str(round(sp, 2)) + ', ' + str(round(se, 2)), size = 7) +\\\n",
    "        theme_bw()\n",
    "    return(plot)\n",
    "\"\"\"\n",
    "def create_roc_plot_with_optimal(all_coords, optimal_threshold):\n",
    "    # Create copy and calculate metrics\n",
    "    all_coords_copy = all_coords.copy()\n",
    "    all_coords_copy['sp'] = all_coords_copy.true_neg/all_coords_copy.neg\n",
    "    all_coords_copy['se'] = all_coords_copy.true_pos/all_coords_copy.pos\n",
    "    \n",
    "    # Get optimal point\n",
    "    best_coords = all_coords_copy[all_coords_copy.thresholds == optimal_threshold]\n",
    "    sp = best_coords.sp.values[0]\n",
    "    se = best_coords.se.values[0]\n",
    "    \n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(all_coords_copy['sp'], all_coords_copy['se'],\n",
    "            color='k', linewidth=0.9)\n",
    "    \n",
    "    # Add optimal point\n",
    "    ax.scatter([sp], [se], color='k', s = 100)\n",
    "    \n",
    "    # Add text annotation\n",
    "    ax.text(sp, se + 0.03,\n",
    "            f\"{sp:.2f}, {se:.2f}\",\n",
    "            fontsize = 9,\n",
    "            ha='center')\n",
    "    ax.text(sp - 0.02, se - 0.18,\n",
    "            'specificity (TNR) \\n& sensitivity (TPR) \\nat the best threshold',\n",
    "            fontsize = 9,\n",
    "            ha='center'\n",
    "           )\n",
    "    \n",
    "    # Set axis ticks and limits\n",
    "    ax.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xticks(np.arange(0, 1.1, 0.1))\n",
    "    ax.set_xlabel('specificity')\n",
    "    ax.set_ylabel('sensitivity')\n",
    "    \n",
    "    # Reverse x-axis\n",
    "    ax.set_xlim(1, 0)\n",
    "    \n",
    "    # Style similar to theme_bw()\n",
    "    ax.grid(True, linestyle='-', alpha=0.2)\n",
    "    ax.set_facecolor('white')\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_color('black')\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.pardir, 'data', 'bisnode_firms_clean.csv') # this will produce a path with the right syntax for your operating system\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORT - FROM FILE\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add utils folder to sys path \n",
    "# Note: os.path.join() creates a string with the right syntax for defining a path for your operating sytem.\n",
    "sys.path.append(os.path.join(repository_path, 'utils'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Define data folder\n",
    "data_path = os.path.join(repository_path, 'data')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the prewritten helper functions\n",
    "from py_helper_functions import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DATA IMPORT - FROM FILE\n",
    "data = pd.read_csv(os.path.join(data_path, 'bisnode_firms_clean.csv'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DATA IMPORT - FROM GITHUB\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/peterduronelly/DA3-Python-Codes/main/data/bisnode_firms_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Current script folder\n",
    "path = os.getcwd()\n",
    "base_dir = path.split(\"da_case_studies\")[0]\n",
    "\n",
    "#Set the location of your data directory\n",
    "data_dir = base_dir + 'da_data_repo'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# location folders\n",
    "data_in = os.path.join(data_dir, 'bisnode-firms/clean/')\n",
    "data_out = os.path.join(data_dir, 'bisnode-firms/')\n",
    "output = os.path.join(data_out, 'output/')\n",
    "func = os.path.join(base_dir, 'da_case_studies/ch00-tech-prep/')\n",
    "sys.path.append(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variable sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawvars = [\"curr_assets\", \"curr_liab\", \"extra_exp\", \"extra_inc\", \"extra_profit_loss\", \"fixed_assets\",\n",
    "              \"inc_bef_tax\", \"intang_assets\", \"inventories\", \"liq_assets\", \"material_exp\", \"personnel_exp\",\n",
    "              \"profit_loss_year\", \"sales\", \"share_eq\", \"subscribed_cap\"]\n",
    "\n",
    "qualityvars = [\"balsheet_flag\", \"balsheet_length\", \"balsheet_notfullyear\"]\n",
    "\n",
    "engvar = [\"total_assets_bs\", \"fixed_assets_bs\", \"liq_assets_bs\", \"curr_assets_bs\",\n",
    "            \"share_eq_bs\", \"subscribed_cap_bs\", \"intang_assets_bs\", \"extra_exp_pl\",\n",
    "            \"extra_inc_pl\", \"extra_profit_loss_pl\", \"inc_bef_tax_pl\", \"inventories_pl\",\n",
    "            \"material_exp_pl\", \"profit_loss_year_pl\", \"personnel_exp_pl\"]\n",
    "\n",
    "engvar2 = [\"extra_profit_loss_pl_quad\", \"inc_bef_tax_pl_quad\",\n",
    "             \"profit_loss_year_pl_quad\", \"share_eq_bs_quad\"]\n",
    "\n",
    "engvar3 = []\n",
    "for col in df.columns:\n",
    "    if col.endswith('flag_low') or col.endswith('flag_high') or col.endswith('flag_error') or col.endswith('flag_zero'):\n",
    "        engvar3.append(col)\n",
    "\n",
    "d1 =  [\"d1_sales_mil_log_mod\", \"d1_sales_mil_log_mod_sq\",\n",
    "         \"flag_low_d1_sales_mil_log\", \"flag_high_d1_sales_mil_log\"]\n",
    "\n",
    "hr = [\"female\", \"ceo_age\", \"flag_high_ceo_age\", \"flag_low_ceo_age\",\n",
    "        \"flag_miss_ceo_age\", \"ceo_count\", \"labor_avg_mod\",\n",
    "        \"flag_miss_labor_avg\", \"foreign_management\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ind2_cat.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `patsy` formula language [here](https://patsy.readthedocs.io/en/latest/formulas.html), treatment of categorical variables [here](https://patsy.readthedocs.io/en/latest/categorical-coding.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat dummy columns from category variables and drop first level\n",
    "ind2_catmat = patsy.dmatrix(\"0 + C(ind2_cat)\", df, return_type=\"dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. \"0 +\" at the start means \"no intercept\" - it suppresses the automatic inclusion of an intercept term (which would be a column of 1s)\n",
    "\n",
    "2. \"C(ind2_cat)\" means:\n",
    "   1. C() is the categorical term operator\n",
    "   2. it will convert the 'ind2_cat' column into dummy variables\n",
    "   3. it creates one column for each unique category in ind2_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2_catmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: why are we dropping the first level? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2_catmat = ind2_catmat.drop(['C(ind2_cat)[26]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.m_region_loc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_region_locmat = patsy.dmatrix(\"0 + C(m_region_loc)\",df, return_type=\"dataframe\")\n",
    "m_region_locmat = m_region_locmat.drop(['C(m_region_loc)[Central]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.urban_m.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urban_mmat = patsy.dmatrix(\"0 + C(urban_m)\",df, return_type=\"dataframe\")\n",
    "urban_mmat = urban_mmat.drop(['C(urban_m)[1]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X1\n",
    "basevars = df[[\"sales_mil_log\", \"sales_mil_log_sq\", \"d1_sales_mil_log_mod\", \"profit_loss_year_pl\"]]\n",
    "X1 = pd.concat([basevars, ind2_catmat], axis=1)\n",
    "\n",
    "# Define X2\n",
    "X2additional_vars = df[[\"fixed_assets_bs\", \"share_eq_bs\",\"curr_liab_bs\", \"curr_liab_bs_flag_high\", \\\n",
    "                          \"curr_liab_bs_flag_error\",  \"age\", \"foreign_management\"]]\n",
    "X2 = pd.concat([X1, X2additional_vars], axis=1)\n",
    "\n",
    "# Define X3\n",
    "firm = pd.concat([df[[\"age\", \"age2\", \"new\"]], ind2_catmat, m_region_locmat, urban_mmat], axis=1)\n",
    "X3 = pd.concat([df[[\"sales_mil_log\", \"sales_mil_log_sq\"] + engvar + d1], firm], axis=1)\n",
    "\n",
    "# Define X4\n",
    "X4 = pd.concat([df[[\"sales_mil_log\", \"sales_mil_log_sq\"] + engvar + d1 \\\n",
    "                                 + engvar2 + engvar3 + hr + qualityvars], firm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define X5\n",
    "\n",
    "#Creat matrix for interactions1 variables\n",
    "int1mat = patsy.dmatrix(\"0 + C(ind2_cat):age + C(ind2_cat):age2 + C(ind2_cat):d1_sales_mil_log_mod \\\n",
    "                + C(ind2_cat):sales_mil_log + C(ind2_cat):ceo_age + C(ind2_cat):foreign_management \\\n",
    "                + C(ind2_cat):female + C(ind2_cat):C(urban_m) + C(ind2_cat):labor_avg_mod\", \n",
    "                        df, return_type=\"dataframe\")\n",
    "\n",
    "#Drop first level to get k-1 dummies out of k categorical levels \n",
    "for col in int1mat.columns:\n",
    "    if col.startswith('C(ind2_cat)[26]') or col.endswith('C(urban_m)[1]'):\n",
    "        int1mat = int1mat.drop([col], axis=1)\n",
    "        \n",
    "#Creat matrix for interactions2 variables        \n",
    "int2mat = patsy.dmatrix(\"0 + sales_mil_log:age + sales_mil_log:female + sales_mil_log:profit_loss_year_pl \\\n",
    "                + sales_mil_log:foreign_management\", \n",
    "                        df, return_type=\"dataframe\")\n",
    "\n",
    "X5 = pd.concat([X4, int1mat, int2mat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logitvars for LASSO\n",
    "logitvars = pd.concat([X4, int1mat, int2mat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rfvars for RF (no interactions, no modified features)\n",
    "rfvars  = pd.concat([df[[\"sales_mil\", \"d1_sales_mil_log\"] + rawvars + hr + qualityvars], firm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear and logistic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simplest model: X1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_modelx1 = LinearRegression().fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results(y, ols_modelx1.predict(X1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_modelx1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_modelx1.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X1, ols_modelx1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_modelx1 = LogisticRegression(\n",
    "    solver = \"newton-cg\", \n",
    "    max_iter = 1000, \n",
    "    penalty = None, \n",
    "    random_state = 20250224).fit(X1, y)\n",
    "regression_results(y, glm_modelx1.predict(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Negative R² means that your model is performing worse than a horizontal line (the mean of the target variable). In other words, your model's predictions are worse than simply guessing the mean value every time.\n",
    "- Negative explained variance similarly indicates that your model's predictions have more variance than the actual data points themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the negative variance: see also Wikipedia on [$R^2$](https://en.wikipedia.org/wiki/Coefficient_of_determination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_modelx1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_modelx1.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X1, glm_modelx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model X2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_modelx2 = LogisticRegression(\n",
    "    solver=\"newton-cg\", \n",
    "    max_iter=1000, \n",
    "    penalty=None).fit(X2, y)\n",
    "regression_results(y, glm_modelx2.predict(X2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X2, glm_modelx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx2 = sm.Logit(y,sm.add_constant(X2)).fit().get_margeff()\n",
    "print(mx2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline model is X4 (all vars, but no interactions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_model = LinearRegression().fit(X4, y)\n",
    "regression_results(y, ols_model.predict(X4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X4, ols_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model = LogisticRegression(\n",
    "    solver = \"newton-cg\",\n",
    "    max_iter = 1000, \n",
    "    penalty= None, verbose = 3).fit(X4, y)\n",
    "regression_results(y, glm_model.predict(X4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(X4, glm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get marginal effects\n",
    "m = sm.Logit(y,sm.add_constant(X4)).fit().get_margeff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Keep significant variables only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = pd.DataFrame(t.data).iloc[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marginal_effects = pd.DataFrame(t.data, columns = columns).iloc[1:].astype(\n",
    "    {'dy/dx': float, 'P>|z|': float})\n",
    "df_marginal_effects[df_marginal_effects['P>|z|'] <= 0.05].sort_values(by = 'dy/dx', ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into train and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_holdout= train_test_split(\n",
    "    df.index.values, train_size=round(0.8*len(df.index)), random_state=42)\n",
    "\n",
    "y_train = y.iloc[index_train]\n",
    "y_holdout = y.iloc[index_holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total')\n",
    "print(df['default'].value_counts(normalize=True), '\\n')\n",
    "print('Train')\n",
    "print(df.iloc[index_train]['default'].value_counts(normalize=True), '\\n')\n",
    "print('Holdout')\n",
    "print(df.iloc[index_holdout]['default'].value_counts(normalize=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions using cross-validations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <font color = ' blue'> A digression into optimization algorithms </font>\n",
    "\n",
    "\n",
    "    \n",
    "Our task is to find the best parameters that give us the least error in predicting the output. We call the function that calculates this error a **cost function**  or a **loss function**, and our goal is to minimize the error in order to get the best-predicted output. This cost function is usally **quadratic**, either globally with one optimum (simple case) or locally with multiple local optima and one global optima somewhere along the sets of parameters.\n",
    "- *newton-cg*: It uses a quadratic function minimalization and finds global optimum swiftly. Drawbacks: computationally expensive and stops at the first saddle point.\n",
    "- *newton-cholesky*: It uses a newton-cg algorithm with a Cholesky decomposition of the Hessian matrix. This solver is more efficient than \"newton-cg\" for large datasets, as it requires less memory and computational resources. However, it may not converge for some datasets.\n",
    "- *liblinear*: It is a linear classification that supports logistic regression and linear support vector machine. It applies automatic parameter selection (a.k.a L1 Regularization) and it’s recommended when you have high dimension dataset (recommended for solving large-scale classification problems).\n",
    "- *lbfgs*: It is a simplified newton-cg which performs better on a limited dataset. It, however, may not converge to anything. \n",
    "‘newton-cholesky’, \n",
    "- *sag* (stochastic average descent): It optimizes the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method's iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values, the SAG method achieves a faster convergence rate than black-box SG methods. Drawbacks: it only supports L2 regularization and it is less practical for large dataset for its memory consumption. \n",
    "- *saga* (stochastic average descent): A sag-alternative which supports L2 regularization.\n",
    "\n",
    "More [here](https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions)\n",
    "\n",
    "Supported penalties by solver:\n",
    "\n",
    "- *lbfgs*: L2\n",
    "- *liblinear*: L1, L2\n",
    "- *newton-cg*: L2\n",
    "- *newton-cholesky*: L2\n",
    "- *sag*: L2\n",
    "- *saga*: elasticnet, L1, L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify cross-validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KFold(n_splits = 5, shuffle = True, random_state = 20250224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no regularisation needed so setting the paremeter to very high value\n",
    "Cs_value_logit = [1e20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using `LogisticRegressionCV`, a logistic regression with built-in cross-validation.\n",
    "\n",
    "Note: Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, *smaller values specify stronger regularization*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model_vars = [X1.iloc[index_train], X2.iloc[index_train], X3.iloc[index_train], X4.iloc[index_train], X5.iloc[index_train]]\n",
    "\n",
    "logit_models = dict()\n",
    "CV_RMSE_folds = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(logit_model_vars)):\n",
    "    print(datetime.datetime.now(), f'Running regression {i}...')\n",
    "    LRCV_brier = LogisticRegressionCV(\n",
    "        Cs = Cs_value_logit, \n",
    "        cv = k, # simply the number of folds\n",
    "        refit = True, \n",
    "        scoring = 'neg_brier_score', \n",
    "        solver = \"newton-cg\", \n",
    "        tol=1e-7, \n",
    "        random_state = 20250224)\n",
    "    logit_models['X'+str(i+1)] = LRCV_brier.fit(logit_model_vars[i], y_train)\n",
    "    \n",
    "    # Calculate RMSE on test for each fold\n",
    "    CV_RMSE_folds['X'+str(i+1)] = np.sqrt(-1*(logit_models['X'+str(i+1)].scores_[1])).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(-1*logit_models['X5'].scores_[1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_RMSE_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_logitvars = pd.DataFrame(StandardScaler().fit_transform(logitvars.iloc[index_train]))\n",
    "normalized_logitvars.columns = logitvars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas=list(10**np.arange(-1,-4.01, -1/3))\n",
    "n_obs = normalized_logitvars.shape[0]*4/5\n",
    "Cs_values = [1/(l*n_obs) for l in lambdas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs_values # the strength of the regularization -> supressing unimportant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for ***accuracy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logLasso = LogisticRegressionCV(\n",
    "    Cs = Cs_values, \n",
    "    penalty = 'l1', # L1 makes it lasso\n",
    "    cv = k, \n",
    "    refit = True, \n",
    "    scoring = 'accuracy', \n",
    "    solver = 'liblinear',\n",
    "    random_state = 20250224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logit_models[\"LASSO\"] = logLasso.fit(normalized_logitvars, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_summary_lasso = cv_summary(lambdas, Cs_values, logit_models[\"LASSO\"])\n",
    "cv_summary_lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lambda = cv_summary_lasso.sort_values('mean_cv_score', ascending = False).iloc[0,0]\n",
    "best_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_coef_matrix(normalized_logitvars, logit_models[\"LASSO\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for ***Brier-score*** (aka RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refit with negative brier score so we have RMSE values for the same cv split\n",
    "\n",
    "logLasso_brier = LogisticRegressionCV(\n",
    "    Cs = Cs_values, \n",
    "    penalty = 'l1', \n",
    "    cv = k, \n",
    "    refit = True, \n",
    "    scoring = 'neg_brier_score', \n",
    "    solver = \"liblinear\", \n",
    "    random_state = 20250224)\n",
    "logLasso_brier_fitted = logLasso_brier.fit(normalized_logitvars, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(lambdas):\n",
    "    if l == best_lambda:\n",
    "        best_lambda_i = i\n",
    "        CV_RMSE_folds['LASSO'] = np.sqrt(-1*(logLasso_brier_fitted.scores_[1][:,i])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_RMSE_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC using no loss fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate AUC for each folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUC_folds = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**refit logit models with AUC so we have AUC values for the same cv split**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for ***AUC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(len(logit_model_vars)):\n",
    "    print(datetime.datetime.now(), f'Running regression {i}...')\n",
    "    LRCV_auc = LogisticRegressionCV(\n",
    "        Cs = Cs_value_logit, \n",
    "        cv = k, \n",
    "        refit = True, \n",
    "        scoring = 'roc_auc', \n",
    "        solver= \"newton-cg\", \n",
    "        tol = 1e-7, \n",
    "        random_state = 20250224)\n",
    "    LRCV_auc_fit = LRCV_auc.fit(logit_model_vars[i], y_train)\n",
    "    # Calculate AUC on test for each fold\n",
    "    CV_AUC_folds['X'+str(i+1)] = LRCV_auc_fit.scores_[1][:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_AUC_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#refit with AUC so we have AUC values for the same cv split\n",
    "\n",
    "logLasso_auc = LogisticRegressionCV(\n",
    "    Cs=Cs_values, \n",
    "    penalty='l1', # this makes it lasso\n",
    "    cv=k, \n",
    "    refit=True, scoring='roc_auc', \n",
    "    solver=\"liblinear\", \n",
    "    random_state = 20250224)\n",
    "\n",
    "logLasso_auc_fitted = logLasso_auc.fit(normalized_logitvars, y_train)\n",
    "\n",
    "CV_AUC_folds['LASSO'] = logLasso_auc_fitted.scores_[1][:,best_lambda_i].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_AUC_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each model: average RMSE and average AUC for models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RMSE = dict()\n",
    "CV_AUC = dict()\n",
    "nvars = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in logit_models:\n",
    "    CV_RMSE[key] = np.mean(CV_RMSE_folds[key])\n",
    "    CV_AUC[key] = np.mean(CV_AUC_folds[key])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in logit_models:\n",
    "    if key != 'LASSO':\n",
    "        nvars[key] = logit_models[key].n_features_in_\n",
    "    else:\n",
    "        nvars[key] = sum(x != 0 for x in logit_models[key].coef_[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in nvars.items():\n",
    "    print(f'{k.ljust(5)}: {str(v).rjust(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We have 6 models, (5 logit and the logit lasso). For each we have a 5-CV RMSE and AUC.\n",
    "We pick our preferred model based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary1 = np.transpose(pd.DataFrame.from_dict([nvars,CV_RMSE,CV_AUC], orient='columns'))\n",
    "logit_summary1.columns = ['Number of predictors', 'CV RMSE', 'CV AUC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take best model and estimate RMSE on holdout\n",
    "X4, X5 and LASSO are practically the same - go with the simplest model.\n",
    "\n",
    "The `predict_proba` method returns *probababilites*, as opposed to the `predict` method (see below), which returns *classes* (trues and falses). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = logit_models['X4']\n",
    "best_model_X_holdout = X4.iloc[index_holdout]\n",
    "logit_predicted_probabilities_holdout = best_model.predict_proba(best_model_X_holdout)[:,1]\n",
    "best_rmse_holdout = np.sqrt(metrics.mean_squared_error(y_holdout, logit_predicted_probabilities_holdout))\n",
    "round(best_rmse_holdout, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_predicted_probabilities_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_holdout, logit_predicted_probabilities_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame({\n",
    "    'fpr': fpr,\n",
    "    'tpr': tpr,\n",
    "    'thresholds': thresholds\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.iloc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,6))\n",
    "ax = sns.lineplot(\n",
    "    data = df_,\n",
    "    x = fpr, \n",
    "    y = tpr)\n",
    "ax.set_title('ROC curve for the best model (X4)')\n",
    "ax.set_xlabel(\"False positive rate (1 - Specificity)\"), \n",
    "ax.set_ylabel(\"True positive rate (Sensitivity)\")\n",
    "ax.set_xticks([x/10 for x in range(0,11,1)])\n",
    "ax.set_yticks([x/10 for x in range(0,11,1)])\n",
    "ax.plot(df_.fpr, df_.fpr, color = 'k', linewidth = 1)\n",
    "ax.grid(True, linestyle=':');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion table with different tresholds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default**: 0.5 is used as threshold to convert probabilities to binary classes. The `predict` method uses 0.5 as threshold to split positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_class_prediction = best_model.predict(best_model_X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_class_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, counts = np.unique(logit_class_prediction.tolist(), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(values[0],' (no default): ',counts[0])\n",
    "print(values[1],' (default): ',counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion matrix: summarize different type of errors and successfully predicted cases   \n",
    "positive = \"yes\": explicitly specify the positive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_object1 = confusion_matrix(y_holdout, logit_class_prediction, labels=[0,1])\n",
    "cm1 = pd.DataFrame(cm_object1, \n",
    "    index=['Actual no default', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying **different thresholds**\n",
    "\n",
    "- 0.5 same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction = np.where(logit_predicted_probabilities_holdout < 0.5, 0, 1)\n",
    "cm_object1b = confusion_matrix(y_holdout, holdout_prediction, labels=[0,1])\n",
    "cm1b = pd.DataFrame(cm_object1b, \n",
    "    index=['Actual no default', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TPR: {round(cm1b.iloc[1,1] / cm1b.iloc[1].sum(), 3)}')\n",
    "print(f'FPR: {round(cm1b.iloc[0,1] / cm1b.iloc[0].sum(), 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "216/(216+574)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a sensible choice: **mean of predicted probabilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predicted_default_prob = np.mean(logit_predicted_probabilities_holdout)\n",
    "round(mean_predicted_default_prob, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_prediction = np.where(logit_predicted_probabilities_holdout < mean_predicted_default_prob, 0, 1)\n",
    "cm_object2 = confusion_matrix(y_holdout, holdout_prediction, labels=[0,1])\n",
    "cm2 = pd.DataFrame(cm_object2, \n",
    "    index=['Actul no defaul', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TPR: {round(cm2.iloc[1,1] / cm2.iloc[1].sum(), 3)}')\n",
    "print(f'FPR: {round(cm2.iloc[0,1] / cm2.iloc[0].sum(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC using a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss function**: relative cost of of a false negative classification (as compared with a false positive classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FP = 1\n",
    "FN = 10\n",
    "cost = FN/FP\n",
    "\n",
    "# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))\n",
    "prevelance = y_train.sum()/len(y_train)\n",
    "print(f'Prevelance: {prevelance:,.3f}')\n",
    "print(f'Simple best threshold estimation: {FP/(FP+FN):,.3f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draw ROC Curve and find optimal threshold with loss function**\n",
    "\n",
    "Instread of with the simple threshold calculation we are running a search to find the thresholds where the loss is the smallest for each model for every fold. \n",
    "\n",
    "The optimal cut-off is the threshold that *maximizes the distance to the identity (diagonal) line*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(logit_models):\n",
    "    print(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = KFold(n_splits = 5, shuffle = True, random_state = 20250224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds_cv = dict()\n",
    "expected_loss_cv = dict()\n",
    "fold5_threshold = dict()\n",
    "fold5_expected_loss = dict()\n",
    "fold5_all_coords = dict()\n",
    "\n",
    "for i, model_name in enumerate(logit_models):\n",
    "    best_thresholds = []\n",
    "    expected_loss =[]\n",
    "    \n",
    "    if model_name != 'LASSO':\n",
    "        X = logit_model_vars[i]\n",
    "        c_index = 0\n",
    "    else:\n",
    "        X = normalized_logitvars\n",
    "        c_index = best_lambda_i\n",
    "    \n",
    "    fold = 0\n",
    "    \n",
    "    for train_index, test_index in k.split(X):\n",
    "        X_fold = X.iloc[test_index,:]\n",
    "        y_fold = y_train.iloc[test_index]\n",
    "        \n",
    "        pred_fold = generate_fold_prediction(logit_models[model_name], X_fold, fold, c_index)\n",
    "    \n",
    "        false_pos_rate, true_pos_rate, thresholds = roc_curve(y_fold, pred_fold)\n",
    "\n",
    "        # this is where we pick the best threshold for minimal loss at each model for every single fold\n",
    "        optimal_threshold = sorted(\n",
    "            list(\n",
    "                zip(\n",
    "                    np.abs(\n",
    "                        true_pos_rate + (1 - prevelance)/(cost * prevelance)*(1-false_pos_rate)\n",
    "                    ),\n",
    "                    thresholds\n",
    "                )\n",
    "            )\n",
    "            , key=lambda i: i[0], reverse=True)[0][1]\n",
    "        \n",
    "        best_thresholds.append(optimal_threshold)\n",
    "        threshold_prediction = np.where(pred_fold < optimal_threshold, 0, 1)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_fold, threshold_prediction, labels=[0,1]).ravel()\n",
    "\n",
    "        # expected loss for each model for every single fold\n",
    "        curr_exp_loss = (fp*FP + fn*FN)/len(y_fold)\n",
    "        expected_loss.append(curr_exp_loss)\n",
    "        fold = fold+1\n",
    "\n",
    "    # best threshold and expected loss for each model (averaging out the folds)\n",
    "    best_thresholds_cv[model_name] = np.mean(best_thresholds)\n",
    "    expected_loss_cv[model_name] = np.mean(expected_loss)\n",
    "\n",
    "    # best threshold and expected loss for each model for fold5 ONLY\n",
    "    fold5_threshold[model_name] = optimal_threshold\n",
    "    fold5_expected_loss[model_name] = curr_exp_loss\n",
    "\n",
    "    all_coords = pd.DataFrame({\n",
    "        'false_pos': false_pos_rate*sum(y_fold == 0),\n",
    "        'true_pos': true_pos_rate*sum(y_fold == 1),\n",
    "        'false_neg': sum(y_fold == 1) - true_pos_rate*sum(y_fold == 1),\n",
    "        'true_neg': sum(y_fold == 0) - false_pos_rate*sum(y_fold == 0),\n",
    "        'pos': sum(y_fold == 1),\n",
    "        'neg': sum(y_fold == 0),\n",
    "        'n': len(y_fold),\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "    \n",
    "    fold5_all_coords[model_name] = all_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_all_coords['X1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary2 = pd.DataFrame(best_thresholds_cv.items(),columns=['Model', 'Avg of optimal thresholds'])\n",
    "logit_summary2['Threshold for Fold5'] = fold5_threshold.values()\n",
    "logit_summary2['Avg expected loss'] = expected_loss_cv.values()\n",
    "logit_summary2['Expected loss for Fold5'] = fold5_expected_loss.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_summary2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss plot based on Fold5 in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_plot = 'X1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_all_coords[model_to_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_threshold[model_to_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_expected_loss[model_to_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_loss_plot(\n",
    "    fold5_all_coords[model_to_plot], \n",
    "    fold5_threshold[model_to_plot], \n",
    "    fold5_expected_loss[model_to_plot]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC plot plot based on Fold5 in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_plot = 'X1'\n",
    "create_roc_plot_with_optimal(fold5_all_coords[model_to_plot], fold5_threshold[model_to_plot]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick best model based on average expected loss\n",
    "\n",
    "### X4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logit_optimal_treshold = best_thresholds_cv[\"X4\"]\n",
    "best_logit_optimal_treshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get expected loss on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_treshold = np.where(logit_predicted_probabilities_holdout < best_logit_optimal_treshold, 0, 1)\n",
    "tn, fp, fn, tp = confusion_matrix(y_holdout, holdout_treshold, labels=[0,1]).ravel()\n",
    "expected_loss_holdout = (fp*FP + fn*FN)/len(y_holdout)\n",
    "round(expected_loss_holdout, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_object3 = confusion_matrix(y_holdout, holdout_treshold, labels=[0,1])\n",
    "cm3 = pd.DataFrame(cm_object3, \n",
    "    index=['Actul no defaul', 'Actual default'], \n",
    "    columns=['Predicted no default', 'Predicted default'])\n",
    "cm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'TPR: {round(cm3.iloc[1,1] / cm3.iloc[1].sum(), 3)}')\n",
    "print(f'FPR: {round(cm3.iloc[0,1] / cm3.iloc[0].sum(), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfvars_train = rfvars.iloc[index_train]\n",
    "rfvars_holdout = rfvars.iloc[index_holdout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfvars_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability forest\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap = True` (default), otherwise the whole dataset is used to build each tree.\n",
    "- `max_features`: number of features to consider when looking for the best split\n",
    "- `min_samples_fit`: the minimum number of samples required`to split an internal node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {'max_features': [5, 6, 7],\n",
    "        'criterion':['gini'],\n",
    "        'min_samples_split': [11, 16]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest = RandomForestClassifier(\n",
    "    random_state=20250224, \n",
    "    n_estimators=50, # this should be higher, but it is kept low in order to control runtime on class\n",
    "    oob_score=True)\n",
    "\n",
    "prob_forest_grid = GridSearchCV(\n",
    "    prob_forest, \n",
    "    grid, \n",
    "    cv = 5, \n",
    "    refit='accuracy', # Refit an estimator using the best found parameters on the whole dataset.\n",
    "                      # For multiple metric evaluation, this needs to be a str denoting the scorer \n",
    "                      # that would be used to find the best parameters for refitting the estimator at the end.\n",
    "    scoring = ['accuracy', 'roc_auc', 'neg_brier_score'], \n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prob_forest_fit = prob_forest_grid.fit(rfvars_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CV summary table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_fit.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grid has 3x2=6 elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_accuracy = np.zeros([6])\n",
    "for i in range(5):\n",
    "    cv_accuracy = cv_accuracy + prob_forest_fit.cv_results_['split' + str(i) + '_test_accuracy']\n",
    "cv_accuracy = cv_accuracy/5\n",
    "cv_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(cv_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_fit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_auc = np.zeros([6])\n",
    "for i in range(5):\n",
    "    cv_auc = cv_auc + prob_forest_fit.cv_results_['split' + str(i) + '_test_roc_auc']\n",
    "cv_auc = cv_auc/5\n",
    "cv_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_rmse = np.zeros([6])\n",
    "for i in range(5):\n",
    "    cv_rmse = cv_rmse +np.sqrt(-1*(prob_forest_fit.cv_results_['split' + str(i) + '_test_neg_brier_score'])).tolist()\n",
    "cv_rmse = cv_rmse/5\n",
    "cv_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_cv_results = pd.DataFrame({\n",
    "    'max_features': prob_forest_fit.cv_results_['param_max_features'],\n",
    "    'min_samples_split': prob_forest_fit.cv_results_['param_min_samples_split'],\n",
    "    'cv_accuracy': cv_accuracy,\n",
    "    'cv_auc': cv_auc,\n",
    "    'cv_rmse': cv_rmse\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_cv_results.style.format(\n",
    "    {\n",
    "        'cv_accuracy': '{:.5f}',\n",
    "        'cv_auc': '{:.5f}',\n",
    "        'cv_rmse': '{:.5f}'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal parameter values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_features = prob_forest_fit.best_params_['max_features']\n",
    "best_min_sample_split = prob_forest_fit.best_params_['min_samples_split']\n",
    "prob_forest_fit.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average RMSE and AUC over folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_best_results = prob_forest_cv_results[\n",
    "    (prob_forest_cv_results.max_features == best_max_features) & \n",
    "    (prob_forest_cv_results.min_samples_split == best_min_sample_split)]\n",
    "prob_forest_best_results_index = prob_forest_best_results.index.values[0]\n",
    "\n",
    "CV_RMSE['rf_p'] = prob_forest_best_results.cv_rmse.values[0]\n",
    "CV_AUC['rf_p'] = prob_forest_best_results.cv_auc.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get fold level RMSE and AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_RMSE_folds_rf_p = list()\n",
    "\n",
    "for i in range(5):\n",
    "    rmse = np.sqrt(-1*(prob_forest_fit.cv_results_['split' + str(i) + '_test_neg_brier_score'])).tolist()[prob_forest_best_results_index]\n",
    "    CV_RMSE_folds_rf_p.append(rmse)\n",
    "\n",
    "CV_RMSE_folds['rf_p'] = CV_RMSE_folds_rf_p\n",
    "\n",
    "CV_AUC_folds_rf_p = list()\n",
    "\n",
    "for i in range(5):\n",
    "    rmse = prob_forest_fit.cv_results_['split' + str(i) + '_test_roc_auc'][prob_forest_best_results_index]\n",
    "    CV_AUC_folds_rf_p.append(rmse)\n",
    "\n",
    "CV_AUC_folds['rf_p'] = CV_AUC_folds_rf_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(CV_AUC_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now use loss function and search for best thresholds and expected loss for each folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = list()\n",
    "expected_loss = list()\n",
    "\n",
    "fold = 0\n",
    "for train_index, test_index in k.split(rfvars_train):\n",
    "    print(f'{datetime.datetime.now()}: fold {fold}...') \n",
    "    X_fold = rfvars_train.iloc[test_index,:]\n",
    "    y_fold = y_train.iloc[test_index]\n",
    "    \n",
    "    X_fold_train = rfvars_train.iloc[train_index,:]\n",
    "    y_fold_train = y_train.iloc[train_index]\n",
    "    \n",
    "    prob_forest_best = RandomForestClassifier(\n",
    "        random_state=20250224, \n",
    "        n_estimators=50, # make it higher in practice (default being 100)\n",
    "        oob_score=True,\n",
    "        criterion = 'gini', \n",
    "        max_features = best_max_features, min_samples_split = best_min_sample_split\n",
    "    )\n",
    "    \n",
    "    prob_forest_best_fold = prob_forest_best.fit(X_fold_train, y_fold_train)\n",
    "    pred_fold = prob_forest_best_fold.predict_proba(X_fold)[:,1]\n",
    "\n",
    "    false_pos_rate, true_pos_rate, threshold = roc_curve(y_fold, pred_fold)\n",
    "    \n",
    "    best_threshold = sorted(\n",
    "        list(\n",
    "            zip(\n",
    "                np.abs(\n",
    "                    true_pos_rate + (1 - prevelance)/(cost * prevelance)*(1-false_pos_rate)\n",
    "                ),\n",
    "                threshold\n",
    "            )\n",
    "        ), \n",
    "        key=lambda x: x[0], reverse=True)[0][1]\n",
    "    \n",
    "    best_thresholds.append(best_threshold)\n",
    "    \n",
    "    threshold_prediction = np.where(pred_fold < best_threshold, 0, 1)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_fold, threshold_prediction, labels=[0,1]).ravel()\n",
    "    curr_exp_loss = (fp*FP + fn*FN)/len(y_fold)\n",
    "    expected_loss.append(curr_exp_loss)\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, t in enumerate(best_thresholds):\n",
    "    print(f'Fold: {f}, best threshold: {t:,.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, l in enumerate(expected_loss):\n",
    "    print(f'Fold: {f}, smalles loss: {l:,.4f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold5_threshold_rf = best_threshold\n",
    "fold5_expected_loss_rf = curr_exp_loss\n",
    "\n",
    "all_coords_rf = pd.DataFrame({\n",
    "    'false_pos': false_pos_rate*sum(y_fold == 0),\n",
    "    'true_pos': true_pos_rate*sum(y_fold == 1),\n",
    "    'false_neg': sum(y_fold == 1) - true_pos_rate*sum(y_fold == 1),\n",
    "    'true_neg': sum(y_fold == 0) - false_pos_rate*sum(y_fold == 0),\n",
    "    'pos': sum(y_fold == 1),\n",
    "    'neg': sum(y_fold == 0),\n",
    "    'n': len(y_fold),\n",
    "    'thresholds': threshold\n",
    "})\n",
    "\n",
    "all_coords_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_loss_cv['rf_p'] = np.mean(expected_loss)\n",
    "best_thresholds_cv['rf_p'] = np.mean(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_summary = pd.DataFrame(\n",
    "    {'CV RMSE': [round(CV_RMSE['rf_p'], 3)],\n",
    "     'CV AUC': [round(CV_AUC['rf_p'], 3)],\n",
    "     'Avg of optimal thresholds': [round(best_thresholds_cv['rf_p'], 3)],\n",
    "     'Threshold for Fold5': [round(best_threshold, 3)],\n",
    "     'Avg expected loss': [round(expected_loss_cv['rf_p'], 3)],\n",
    "     'Expected loss for Fold5': [round(curr_exp_loss, 3)]})\n",
    "\n",
    "rf_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots based on Fold5 in CV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_loss_plot(all_coords_rf, fold5_threshold_rf, fold5_expected_loss_rf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_roc_plot_with_optimal(all_coords_rf, fold5_threshold_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take model to holdout and estimate RMSE, AUC and expected loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_fit_best = prob_forest_fit.best_estimator_\n",
    "rf_predicted_probabilities_holdout = prob_forest_fit_best.predict_proba(rfvars_holdout)[:,1]\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_holdout, rf_predicted_probabilities_holdout))\n",
    "round(rmse_rf, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC AUC  on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_rf = roc_auc_score(y_holdout, rf_predicted_probabilities_holdout)\n",
    "round(auc_rf, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected loss on holdout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_treshold = np.where(rf_predicted_probabilities_holdout < best_thresholds_cv['rf_p'], 0, 1)\n",
    "tn, fp, fn, tp = confusion_matrix(y_holdout, holdout_treshold, labels=[0,1]).ravel()\n",
    "expected_loss_holdout = (fp*FP + fn*FN)/len(y_holdout)\n",
    "round(expected_loss_holdout, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting probalbilities and classifications from RandomforestClassifier\n",
    "\n",
    "- classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_best_fold.predict(rfvars_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_forest_best_fold.predict_proba(rfvars_holdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvars['rf_p'] = len(rfvars.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = pd.DataFrame({\"Model\": list(nvars.keys()),\n",
    "                              \"Number of predictors\": list(nvars.values()),\n",
    "                              \"CV RMSE\": list(CV_RMSE.values()),\n",
    "                              \"CV AUC\": list(CV_AUC.values()),\n",
    "                              \"CV threshold\": list(best_thresholds_cv.values()),\n",
    "                              \"CV expected Loss\": list(expected_loss_cv.values())\n",
    "                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results.style.format(\n",
    "    {\n",
    "        'CV threshold': '{:.4f}',\n",
    "        'CV AUC': '{:.4f}',\n",
    "        'CV RMSE': '{:.4f}',\n",
    "        'CV expected Loss': '{:.4f}'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
